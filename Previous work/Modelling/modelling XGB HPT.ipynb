{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"combined_simulated.csv\"  # Update this path if needed\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio per PC: [0.31493719 0.14862621 0.09723175 0.06805881 0.06457052]\n",
      "Cumulative Explained Variance: [0.31493719 0.4635634  0.56079515 0.62885396 0.69342448]\n",
      "\n",
      "Top 10 most important features for PC1:\n",
      "EXPECTED_FOB          0.305612\n",
      "FUEL_IN_TANKS         0.300676\n",
      "VALUE_FOB             0.300662\n",
      "VALUE_FUEL_QTY_LXT    0.299884\n",
      "VALUE_FUEL_QTY_RXT    0.297503\n",
      "TOTAL_FUEL_LW         0.297184\n",
      "TOTAL_FUEL_RW         0.294962\n",
      "FUEL_USED_4           0.243927\n",
      "TOTAL_FUEL_USED       0.243720\n",
      "FUEL_USED_3           0.243708\n",
      "Name: PC1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_pca(file_path, n_components=5):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Drop non-numeric columns\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_numeric)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "    # Create a DataFrame with principal components\n",
    "    pca_df = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "    # Explained variance ratio\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(\"Explained Variance Ratio per PC:\", explained_variance)\n",
    "    print(\"Cumulative Explained Variance:\", np.cumsum(explained_variance))\n",
    "\n",
    "    # Get feature importance (absolute values of PCA component loadings)\n",
    "    loadings = pd.DataFrame(pca.components_, columns=df_numeric.columns, index=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "    # Rank features by their absolute contribution to PC1\n",
    "    feature_importance = loadings.loc[\"PC1\"].abs().sort_values(ascending=False)\n",
    "\n",
    "    # Top 10 most important features\n",
    "    top_10_features = feature_importance.head(10)\n",
    "    print(\"\\nTop 10 most important features for PC1:\")\n",
    "    print(top_10_features)\n",
    "\n",
    "    return top_10_features\n",
    "\n",
    "# Run PCA and get top 10 features\n",
    "\n",
    "top_features = run_pca(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 376554 entries, 0 to 376553\n",
      "Data columns (total 35 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   UTC_TIME                   376554 non-null  object \n",
      " 1   FUEL_USED_2                376554 non-null  float64\n",
      " 2   FUEL_USED_3                376554 non-null  float64\n",
      " 3   FUEL_USED_4                376554 non-null  float64\n",
      " 4   FW_GEO_ALTITUDE            376554 non-null  float64\n",
      " 5   VALUE_FOB                  376554 non-null  float64\n",
      " 6   VALUE_FUEL_QTY_CT          376554 non-null  float64\n",
      " 7   VALUE_FUEL_QTY_FT1         376554 non-null  float64\n",
      " 8   VALUE_FUEL_QTY_FT2         376554 non-null  float64\n",
      " 9   VALUE_FUEL_QTY_FT3         376554 non-null  float64\n",
      " 10  VALUE_FUEL_QTY_FT4         376554 non-null  float64\n",
      " 11  VALUE_FUEL_QTY_LXT         376554 non-null  float64\n",
      " 12  VALUE_FUEL_QTY_RXT         376554 non-null  float64\n",
      " 13  FLIGHT_PHASE_COUNT         376554 non-null  float64\n",
      " 14  FUEL_USED_1                376554 non-null  float64\n",
      " 15  Flight                     376554 non-null  float64\n",
      " 16  MSN                        376554 non-null  object \n",
      " 17  NEW_FLIGHT                 376554 non-null  bool   \n",
      " 18  FLIGHT_INSTANCE            376554 non-null  int64  \n",
      " 19  FLIGHT_ID                  376554 non-null  object \n",
      " 20  START_FOB                  376554 non-null  float64\n",
      " 21  TOTAL_FUEL_USED            376554 non-null  float64\n",
      " 22  EXPECTED_FOB               376554 non-null  float64\n",
      " 23  FOB_DIFFERENCE             376554 non-null  float64\n",
      " 24  FOB_CHANGE                 376554 non-null  float64\n",
      " 25  EXPECTED_FOB_CHANGE        376554 non-null  float64\n",
      " 26  FUEL_LEAK_RATE             376554 non-null  float64\n",
      " 27  TOTAL_FUEL_LW              376554 non-null  float64\n",
      " 28  TOTAL_FUEL_RW              376554 non-null  float64\n",
      " 29  LW_RW_DIFF                 376554 non-null  float64\n",
      " 30  FUEL_IN_TANKS              376554 non-null  float64\n",
      " 31  CALC_VALUE_FOB_DIFF        376554 non-null  float64\n",
      " 32  START_FOB_VS_FOB_FUELUSED  376554 non-null  float64\n",
      " 33  ALTITUDE_DIFF              376554 non-null  float64\n",
      " 34  LEAK_FLOW_FLAG             376554 non-null  int64  \n",
      "dtypes: bool(1), float64(29), int64(2), object(3)\n",
      "memory usage: 98.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert UTC_TIME to datetime format if it's not already\n",
    "df['UTC_TIME'] = pd.to_datetime(df['UTC_TIME'])\n",
    "\n",
    "# Sort by FLIGHT_ID and UTC_TIME\n",
    "df_sorted = df.sort_values(by=['FLIGHT_ID', 'UTC_TIME'])\n",
    "\n",
    "# Reset index\n",
    "df_sorted = df_sorted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['FLIGHT_PHASE_COUNT','Flight','FLIGHT_INSTANCE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Leak Count: 33475\n",
      "Train set leak rate: 8.2841% == Training Leak Count: 26515 (Expected: 28453) \n",
      "Test set leak rate: 12.3221% == Testing Leak Count: 6960 (Expected: 5022)\n",
      "Training Data Shape: (320070, 32), Testing Data Shape: (56484, 32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test(df, leak_column=\"LEAK_FLOW_FLAG\", test_size=0.15):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train and test sets while maintaining the chronological order.\n",
    "    Ensures the same proportion of leaks in train and test sets.\n",
    "    \"\"\"\n",
    "    #df = df.sort_values(by=\"UTC_TIME\")  # Sort by time to prevent data leakage\n",
    "\n",
    "    split_idx = int(len(df) * (1 - test_size))  # Compute split index\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    test_df = df.iloc[split_idx:]\n",
    "\n",
    "    # Count the number of leaks in each dataset\n",
    "    train_leak_count = train_df['LEAK_FLOW_FLAG'].sum()\n",
    "    test_leak_count = test_df['LEAK_FLOW_FLAG'].sum()\n",
    "    total_leak_count = df['LEAK_FLOW_FLAG'].sum()\n",
    "    # Compute expected leak split (should be 75%-25%)\n",
    "    expected_train_leaks = int(total_leak_count * 0.85)\n",
    "    expected_test_leaks = total_leak_count - expected_train_leaks\n",
    "\n",
    "    # Display class distributions\n",
    "    print(f\"Total Leak Count: {total_leak_count}\")\n",
    "    print(f\"Train set leak rate: {100 * train_df[leak_column].mean():.4f}% == Training Leak Count: {train_leak_count} (Expected: {expected_train_leaks}) \")\n",
    "    print(f\"Test set leak rate: {100 * test_df[leak_column].mean():.4f}% == Testing Leak Count: {test_leak_count} (Expected: {expected_test_leaks})\")\n",
    "    print(f\"Training Data Shape: {train_df.shape}, Testing Data Shape: {test_df.shape}\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Apply split\n",
    "train_df, test_df = split_train_test(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 0.0402 - val_loss: 0.0120\n",
      "Epoch 2/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0088 - val_loss: 0.0071\n",
      "Epoch 3/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 4/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 5/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 6/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 7/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0066 - val_loss: 0.0064\n",
      "Epoch 8/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 9/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 3ms/step - loss: 0.0064 - val_loss: 0.0055\n",
      "Epoch 10/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 843us/step - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 11/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 960us/step - loss: 0.0063 - val_loss: 0.0054\n",
      "Epoch 12/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0063 - val_loss: 0.0068\n",
      "Epoch 13/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 14/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 15/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0050\n",
      "Epoch 17/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 18/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 19/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 20/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0063\n",
      "Epoch 21/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 22/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 23/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0051\n",
      "Epoch 24/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0057\n",
      "Epoch 25/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0049\n",
      "Epoch 26/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0059 - val_loss: 0.0062\n",
      "Epoch 27/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 28/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0052\n",
      "Epoch 29/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 30/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 31/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 32/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 33/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 34/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 35/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0050\n",
      "Epoch 36/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 37/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 38/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0051\n",
      "Epoch 39/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 40/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 41/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0069\n",
      "Epoch 42/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 43/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0078\n",
      "Epoch 44/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0071\n",
      "Epoch 45/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 46/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 47/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 0.0061 - val_loss: 0.0105\n",
      "Epoch 48/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 49/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0062 - val_loss: 0.0067\n",
      "Epoch 50/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0062 - val_loss: 0.0066\n",
      "\u001b[1m1766/1766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 776us/step\n",
      "Confusion Matrix:\n",
      "[[47046  2478]\n",
      " [ 6613   347]]\n",
      "Accuracy: 0.8390517668720345\n",
      "Precision: 0.12283185840707965\n",
      "Recall: 0.04985632183908046\n",
      "F1 Score: 0.07092488502810425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "# Filter out the entries with leaks from training data\n",
    "train_data = train_df[train_df['LEAK_FLOW_FLAG'] == 0].drop(columns=['LEAK_FLOW_FLAG'])\n",
    "\n",
    "# Identify numerical columns and scale only those\n",
    "numerical_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data[numerical_cols])\n",
    "\n",
    "# Define the autoencoder model architecture\n",
    "input_dim = train_data_scaled.shape[1]  # Number of features\n",
    "encoding_dim = 14  # Dimension of the encoded representation\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "# Encoder layers\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "# Decoder layers\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder on the scaled training data\n",
    "autoencoder.fit(train_data_scaled, train_data_scaled,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=False,\n",
    "                validation_split=0.2,\n",
    "                verbose=1)\n",
    "\n",
    "# Prepare the test data by selecting only numerical columns and scaling\n",
    "test_data = test_df[numerical_cols]\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Get the reconstruction loss (MSE) on the test data\n",
    "reconstructions = autoencoder.predict(test_data_scaled)\n",
    "mse = np.mean(np.power(test_data_scaled - reconstructions, 2), axis=1)\n",
    "\n",
    "# Determine the threshold for anomaly detection based on 95th percentile of MSE\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "# Detect anomalies based on the threshold\n",
    "test_df = test_df.copy()  # Avoid SettingWithCopyWarning\n",
    "test_df['reconstruction_error'] = mse\n",
    "test_df['anomaly'] = test_df['reconstruction_error'] > threshold\n",
    "\n",
    "# Calculate evaluation metrics: Confusion Matrix, Accuracy, Precision, Recall, F1 Score\n",
    "y_true = test_df['LEAK_FLOW_FLAG']  # True labels\n",
    "y_pred = test_df['anomaly'].astype(int)  # Predicted labels (0 or 1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)  # Confusion Matrix\n",
    "accuracy = accuracy_score(y_true, y_pred)  # Accuracy\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)  # Precision\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)  # Recall\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)  # F1 Score\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix:\n",
      " [[286917   6638]\n",
      " [ 25152   1363]]\n",
      "Testing Confusion Matrix:\n",
      " [[48240  1284]\n",
      " [ 6892    68]]\n",
      "Accuracy: 0.8552510445435876\n",
      "Recall: 0.009770114942528735\n",
      "Precision: 0.05029585798816568\n",
      "F1 Score: 0.016361886429258902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Remove unwanted columns from training and test data\n",
    "test_df = test_df.drop(columns=['reconstruction_error'], errors='ignore')\n",
    "train_df = train_df.drop(columns=['reconstruction_error'], errors='ignore')\n",
    "\n",
    "# Extract numerical features for training and testing\n",
    "train_x = train_df.select_dtypes(include=[np.number]).drop(columns=['LEAK_FLOW_FLAG'])\n",
    "test_x = test_df.select_dtypes(include=[np.number]).drop(columns=['LEAK_FLOW_FLAG'])\n",
    "\n",
    "# Create an Isolation Forest model\n",
    "isolation_forest = IsolationForest(contamination=0.025, random_state=42)\n",
    "\n",
    "# Fit the model on training data\n",
    "isolation_forest.fit(train_x)\n",
    "\n",
    "# Predict anomalies\n",
    "train_x_predictions = isolation_forest.predict(train_x)\n",
    "test_x_predictions = isolation_forest.predict(test_x)\n",
    "\n",
    "# Convert predictions to binary labels (-1 indicates an anomaly)\n",
    "train_x_predictions = (train_x_predictions == -1).astype(int)\n",
    "test_x_predictions = (test_x_predictions == -1).astype(int)\n",
    "\n",
    "# Print the confusion matrix for training data\n",
    "cm_train = confusion_matrix(train_df['LEAK_FLOW_FLAG'], train_x_predictions)\n",
    "print(\"Training Confusion Matrix:\\n\", cm_train)\n",
    "\n",
    "# Print the confusion matrix for test data\n",
    "cm_test = confusion_matrix(test_df['LEAK_FLOW_FLAG'], test_x_predictions)\n",
    "print(\"Testing Confusion Matrix:\\n\", cm_test)\n",
    "\n",
    "# Print precision, recall, accuracy, and F1 score\n",
    "accuracy = accuracy_score(test_df['LEAK_FLOW_FLAG'], test_x_predictions)\n",
    "recall = recall_score(test_df['LEAK_FLOW_FLAG'], test_x_predictions, zero_division=0)\n",
    "precision = precision_score(test_df['LEAK_FLOW_FLAG'], test_x_predictions, zero_division=0)\n",
    "f1 = f1_score(test_df['LEAK_FLOW_FLAG'], test_x_predictions, zero_division=0)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:35:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training Confusion Matrix:\n",
      " [[287860   5695]\n",
      " [ 16250  10265]]\n",
      "Testing Confusion Matrix:\n",
      " [[49232   292]\n",
      " [ 6635   325]]\n",
      "Accuracy: 0.8773635011684725\n",
      "Recall: 0.04669540229885057\n",
      "Precision: 0.526742301458671\n",
      "F1 Score: 0.0857859311072984\n",
      "               Feature  Importance\n",
      "5        TOTAL_FUEL_RW    0.146212\n",
      "4   VALUE_FUEL_QTY_LXT    0.127165\n",
      "2   VALUE_FUEL_QTY_RXT    0.124556\n",
      "3        TOTAL_FUEL_LW    0.119816\n",
      "7          FUEL_USED_4    0.099216\n",
      "6        FUEL_IN_TANKS    0.079493\n",
      "1         EXPECTED_FOB    0.073321\n",
      "9          FUEL_USED_3    0.066042\n",
      "8      TOTAL_FUEL_USED    0.061626\n",
      "11              lagged    0.041035\n",
      "0            VALUE_FOB    0.035894\n",
      "10      VALUE_FOB_mean    0.025626\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Feature selection for XGBoost\n",
    "drop_columns = ['UTC_TIME', 'FLIGHT_ID', 'MSN']\n",
    "selected_features = [\n",
    "    'VALUE_FOB', 'EXPECTED_FOB', 'VALUE_FUEL_QTY_RXT', 'TOTAL_FUEL_LW',\n",
    "    'VALUE_FUEL_QTY_LXT', 'TOTAL_FUEL_RW', 'FUEL_IN_TANKS', 'LEAK_FLOW_FLAG','FUEL_USED_4','TOTAL_FUEL_USED','FUEL_USED_3']\n",
    "\n",
    "          \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Ensure rolling features exist before selecting features\n",
    "train_df['VALUE_FOB_mean'] = train_df['VALUE_FOB'].rolling(window=10).mean()\n",
    "train_df['VALUE_FOB_mean'] = train_df['VALUE_FOB_mean'].fillna(train_df['VALUE_FOB_mean'].mean())\n",
    "train_df['lagged'] = train_df['VALUE_FOB'].shift(1).fillna(train_df['VALUE_FOB'].mean())\n",
    "test_df['VALUE_FOB_mean'] = test_df['VALUE_FOB'].rolling(window=10).mean()\n",
    "test_df['VALUE_FOB_mean'] = test_df['VALUE_FOB_mean'].fillna(test_df['VALUE_FOB_mean'].mean())\n",
    "test_df['lagged'] = test_df['VALUE_FOB'].shift(1).fillna(test_df['VALUE_FOB'].mean())\n",
    "\n",
    "# Add rolling features to selected features\n",
    "selected_features.extend(['VALUE_FOB_mean', 'lagged'])\n",
    "\n",
    "xgboostdata = train_df[selected_features]\n",
    "\n",
    "# Ensure test set contains all necessary features\n",
    "X_train = xgboostdata.drop(columns=['LEAK_FLOW_FLAG'])\n",
    "y_train = xgboostdata['LEAK_FLOW_FLAG']\n",
    "X_test = test_df[selected_features].drop(columns=['LEAK_FLOW_FLAG'], errors='ignore')\n",
    "y_test = test_df['LEAK_FLOW_FLAG']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "train_x_predictions = xgb.predict(X_train_scaled)\n",
    "test_x_predictions = xgb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "cm_train = confusion_matrix(y_train, train_x_predictions)\n",
    "cm_test = confusion_matrix(y_test, test_x_predictions)\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_x_predictions)\n",
    "recall = recall_score(y_test, test_x_predictions, zero_division=0)\n",
    "precision = precision_score(y_test, test_x_predictions, zero_division=0)\n",
    "f1 = f1_score(y_test, test_x_predictions, zero_division=0)\n",
    "\n",
    "print(\"Training Confusion Matrix:\\n\", cm_train)\n",
    "print(\"Testing Confusion Matrix:\\n\", cm_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = xgb.feature_importances_\n",
    "features = X_train.columns\n",
    "feature_importances_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importances_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:57:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 150, 'subsample': 0.7}\n",
      "Best Score: 0.9171993626394226\n",
      "Final Training Confusion Matrix:\n",
      " [[293509     46]\n",
      " [ 26438     77]]\n",
      "Final Testing Confusion Matrix:\n",
      " [[49447    77]\n",
      " [ 6872    88]]\n",
      "Final Accuracy: 0.8769740103392111\n",
      "Final Recall: 0.01264367816091954\n",
      "Final Precision: 0.5333333333333333\n",
      "Final F1 Score: 0.024701754385964912\n",
      "               Feature  Importance\n",
      "5        TOTAL_FUEL_RW    0.128527\n",
      "9          FUEL_USED_3    0.121194\n",
      "4   VALUE_FUEL_QTY_LXT    0.107299\n",
      "8      TOTAL_FUEL_USED    0.103352\n",
      "3        TOTAL_FUEL_LW    0.101775\n",
      "7          FUEL_USED_4    0.093400\n",
      "2   VALUE_FUEL_QTY_RXT    0.082270\n",
      "1         EXPECTED_FOB    0.081267\n",
      "0            VALUE_FOB    0.061135\n",
      "11              lagged    0.054027\n",
      "6        FUEL_IN_TANKS    0.047438\n",
      "10      VALUE_FOB_mean    0.018317\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameter tuning\n",
    "tuning_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [2, 4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=XGBClassifier(n_estimator=100), param_grid=tuning_params, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Predict with the best model\n",
    "train_x_predictions = grid_search.best_estimator_.predict(X_train_scaled)\n",
    "test_x_predictions = grid_search.best_estimator_.predict(X_test_scaled)\n",
    "\n",
    "# Final evaluation\n",
    "cm_train = confusion_matrix(y_train, train_x_predictions)\n",
    "cm_test = confusion_matrix(y_test, test_x_predictions)\n",
    "accuracy = accuracy_score(y_test, test_x_predictions)\n",
    "recall = recall_score(y_test, test_x_predictions, zero_division=0)\n",
    "precision = precision_score(y_test, test_x_predictions, zero_division=0)\n",
    "f1 = f1_score(y_test, test_x_predictions, zero_division=0)\n",
    "\n",
    "print(\"Final Training Confusion Matrix:\\n\", cm_train)\n",
    "print(\"Final Testing Confusion Matrix:\\n\", cm_test)\n",
    "print(f'Final Accuracy: {accuracy}')\n",
    "print(f'Final Recall: {recall}')\n",
    "print(f'Final Precision: {precision}')\n",
    "print(f'Final F1 Score: {f1}')\n",
    "\n",
    "# Print final feature importance\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importances_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Hyperparameter tuning\n",
    "tuning_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [2, 4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=XGBClassifier(), param_grid=tuning_params, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Predict with the best model\n",
    "train_x_predictions = grid_search.best_estimator_.predict(X_train_scaled)\n",
    "test_x_predictions = grid_search.best_estimator_.predict(X_test_scaled)\n",
    "\n",
    "# Final evaluation\n",
    "cm_train = confusion_matrix(y_train, train_x_predictions)\n",
    "cm_test = confusion_matrix(y_test, test_x_predictions)\n",
    "accuracy = accuracy_score(y_test, test_x_predictions)\n",
    "recall = recall_score(y_test, test_x_predictions, zero_division=0)\n",
    "precision = precision_score(y_test, test_x_predictions, zero_division=0)\n",
    "f1 = f1_score(y_test, test_x_predictions, zero_division=0)\n",
    "\n",
    "print(\"Final Training Confusion Matrix:\\n\", cm_train)\n",
    "print(\"Final Testing Confusion Matrix:\\n\", cm_test)\n",
    "print(f'Final Accuracy: {accuracy}')\n",
    "print(f'Final Recall: {recall}')\n",
    "print(f'Final Precision: {precision}')\n",
    "print(f'Final F1 Score: {f1}')\n",
    "\n",
    "# Print final feature importance\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importances_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UTC_TIME', 'FUEL_USED_2', 'FUEL_USED_3', 'FUEL_USED_4',\n",
       "       'FW_GEO_ALTITUDE', 'VALUE_FOB', 'VALUE_FUEL_QTY_CT',\n",
       "       'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3',\n",
       "       'VALUE_FUEL_QTY_FT4', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT',\n",
       "       'FUEL_USED_1', 'MSN', 'NEW_FLIGHT', 'FLIGHT_ID', 'START_FOB',\n",
       "       'TOTAL_FUEL_USED', 'EXPECTED_FOB', 'FOB_DIFFERENCE', 'FOB_CHANGE',\n",
       "       'EXPECTED_FOB_CHANGE', 'FUEL_LEAK_RATE', 'TOTAL_FUEL_LW',\n",
       "       'TOTAL_FUEL_RW', 'LW_RW_DIFF', 'FUEL_IN_TANKS', 'CALC_VALUE_FOB_DIFF',\n",
       "       'START_FOB_VS_FOB_FUELUSED', 'ALTITUDE_DIFF', 'LEAK_FLOW_FLAG',\n",
       "       'VALUE_FOB_mean', 'lagged'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - loss: 0.0229 - val_loss: 0.0011\n",
      "Epoch 2/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 8ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 3/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 6.2579e-04 - val_loss: 0.0020\n",
      "Epoch 4/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 5.2720e-04 - val_loss: 0.0023\n",
      "Epoch 5/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 4.7345e-04 - val_loss: 0.0023\n",
      "Epoch 6/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 8ms/step - loss: 4.3821e-04 - val_loss: 0.0024\n",
      "Epoch 7/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 4.1150e-04 - val_loss: 0.0024\n",
      "Epoch 8/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 3.8938e-04 - val_loss: 0.0024\n",
      "Epoch 9/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 3.6577e-04 - val_loss: 0.0029\n",
      "Epoch 10/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - loss: 3.5345e-04 - val_loss: 0.0028\n",
      "Epoch 11/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 3.3416e-04 - val_loss: 0.0029\n",
      "Epoch 12/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 3.2211e-04 - val_loss: 0.0027\n",
      "Epoch 13/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - loss: 3.1147e-04 - val_loss: 0.0030\n",
      "Epoch 14/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - loss: 2.9841e-04 - val_loss: 0.0029\n",
      "Epoch 15/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - loss: 2.8394e-04 - val_loss: 0.0032\n",
      "Epoch 16/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - loss: 2.5886e-04 - val_loss: 0.0031\n",
      "Epoch 17/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 2.5157e-04 - val_loss: 0.0032\n",
      "Epoch 18/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 2.3092e-04 - val_loss: 0.0032\n",
      "Epoch 19/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 2.3136e-04 - val_loss: 0.0033\n",
      "Epoch 20/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 2.1929e-04 - val_loss: 0.0034\n",
      "Epoch 21/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 16ms/step - loss: 2.0954e-04 - val_loss: 0.0031\n",
      "Epoch 22/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 33ms/step - loss: 2.0414e-04 - val_loss: 0.0033\n",
      "Epoch 23/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 26ms/step - loss: 1.9919e-04 - val_loss: 0.0032\n",
      "Epoch 24/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 26ms/step - loss: 1.9712e-04 - val_loss: 0.0032\n",
      "Epoch 25/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - loss: 1.8656e-04 - val_loss: 0.0033\n",
      "Epoch 26/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 1.8273e-04 - val_loss: 0.0034\n",
      "Epoch 27/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - loss: 1.8374e-04 - val_loss: 0.0032\n",
      "Epoch 28/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - loss: 1.7245e-04 - val_loss: 0.0034\n",
      "Epoch 29/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 1.6780e-04 - val_loss: 0.0034\n",
      "Epoch 30/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - loss: 1.6629e-04 - val_loss: 0.0033\n",
      "Epoch 31/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - loss: 1.6313e-04 - val_loss: 0.0032\n",
      "Epoch 32/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - loss: 1.6141e-04 - val_loss: 0.0035\n",
      "Epoch 33/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 1.5620e-04 - val_loss: 0.0033\n",
      "Epoch 34/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 1.5492e-04 - val_loss: 0.0035\n",
      "Epoch 35/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - loss: 1.5460e-04 - val_loss: 0.0034\n",
      "Epoch 36/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 1.4835e-04 - val_loss: 0.0034\n",
      "Epoch 37/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 1.4679e-04 - val_loss: 0.0035\n",
      "Epoch 38/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 1.3933e-04 - val_loss: 0.0033\n",
      "Epoch 39/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 1.3913e-04 - val_loss: 0.0035\n",
      "Epoch 40/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 1.3611e-04 - val_loss: 0.0036\n",
      "Epoch 41/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.3413e-04 - val_loss: 0.0035\n",
      "Epoch 42/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 1.3118e-04 - val_loss: 0.0035\n",
      "Epoch 43/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 1.2843e-04 - val_loss: 0.0037\n",
      "Epoch 44/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 1.2402e-04 - val_loss: 0.0034\n",
      "Epoch 45/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5ms/step - loss: 1.2129e-04 - val_loss: 0.0036\n",
      "Epoch 46/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 1.2716e-04 - val_loss: 0.0035\n",
      "Epoch 47/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 1.2448e-04 - val_loss: 0.0036\n",
      "Epoch 48/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 1.1708e-04 - val_loss: 0.0035\n",
      "Epoch 49/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 1.1608e-04 - val_loss: 0.0036\n",
      "Epoch 50/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - loss: 1.2037e-04 - val_loss: 0.0036\n",
      "\u001b[1m1766/1766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "Confusion Matrix:\n",
      "[[46894  2630]\n",
      " [ 6765   195]]\n",
      "Accuracy: 0.8336697117767864\n",
      "Precision: 0.06902654867256637\n",
      "Recall: 0.028017241379310345\n",
      "F1 Score: 0.0398569238630557\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "# Identify numerical columns and scale only those\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_df[numerical_cols])\n",
    "test_data_scaled = scaler.transform(test_df[numerical_cols])\n",
    "\n",
    "# Reshape data for LSTM (samples, timesteps, features)\n",
    "time_steps = 10  # Number of time steps for LSTM input\n",
    "train_data_reshaped = train_data_scaled.reshape((train_data_scaled.shape[0], 1, train_data_scaled.shape[1]))\n",
    "test_data_reshaped = test_data_scaled.reshape((test_data_scaled.shape[0], 1, test_data_scaled.shape[1]))\n",
    "\n",
    "# Define LSTM Autoencoder model\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='relu', input_shape=(1, train_data_scaled.shape[1]), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='relu', return_sequences=False),\n",
    "    RepeatVector(1),\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(train_data_scaled.shape[1]))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the LSTM Autoencoder\n",
    "model.fit(train_data_reshaped, train_data_reshaped,\n",
    "          epochs=50,\n",
    "          batch_size=64,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          verbose=1)\n",
    "\n",
    "# Get reconstruction loss (MSE) on the test data\n",
    "reconstructions = model.predict(test_data_reshaped)\n",
    "mse = np.mean(np.power(test_data_reshaped - reconstructions, 2), axis=(1, 2))\n",
    "\n",
    "# Determine anomaly threshold based on 95th percentile\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "# Detect anomalies based on the threshold\n",
    "test_df = test_df.copy()\n",
    "test_df['reconstruction_error'] = mse\n",
    "test_df['anomaly'] = test_df['reconstruction_error'] > threshold\n",
    "\n",
    "# Evaluation metrics\n",
    "y_true = test_df['LEAK_FLOW_FLAG']\n",
    "y_pred = test_df['anomaly'].astype(int)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
