{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"combined_simulated.csv\"  # Update this path if needed\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 376554 entries, 0 to 376553\n",
      "Data columns (total 35 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   UTC_TIME                   376554 non-null  object \n",
      " 1   FUEL_USED_2                376554 non-null  float64\n",
      " 2   FUEL_USED_3                376554 non-null  float64\n",
      " 3   FUEL_USED_4                376554 non-null  float64\n",
      " 4   FW_GEO_ALTITUDE            376554 non-null  float64\n",
      " 5   VALUE_FOB                  376554 non-null  float64\n",
      " 6   VALUE_FUEL_QTY_CT          376554 non-null  float64\n",
      " 7   VALUE_FUEL_QTY_FT1         376554 non-null  float64\n",
      " 8   VALUE_FUEL_QTY_FT2         376554 non-null  float64\n",
      " 9   VALUE_FUEL_QTY_FT3         376554 non-null  float64\n",
      " 10  VALUE_FUEL_QTY_FT4         376554 non-null  float64\n",
      " 11  VALUE_FUEL_QTY_LXT         376554 non-null  float64\n",
      " 12  VALUE_FUEL_QTY_RXT         376554 non-null  float64\n",
      " 13  FLIGHT_PHASE_COUNT         376554 non-null  float64\n",
      " 14  FUEL_USED_1                376554 non-null  float64\n",
      " 15  Flight                     376554 non-null  float64\n",
      " 16  MSN                        376554 non-null  object \n",
      " 17  NEW_FLIGHT                 376554 non-null  bool   \n",
      " 18  FLIGHT_INSTANCE            376554 non-null  int64  \n",
      " 19  FLIGHT_ID                  376554 non-null  object \n",
      " 20  START_FOB                  376554 non-null  float64\n",
      " 21  TOTAL_FUEL_USED            376554 non-null  float64\n",
      " 22  EXPECTED_FOB               376554 non-null  float64\n",
      " 23  FOB_DIFFERENCE             376554 non-null  float64\n",
      " 24  FOB_CHANGE                 376554 non-null  float64\n",
      " 25  EXPECTED_FOB_CHANGE        376554 non-null  float64\n",
      " 26  FUEL_LEAK_RATE             376554 non-null  float64\n",
      " 27  TOTAL_FUEL_LW              376554 non-null  float64\n",
      " 28  TOTAL_FUEL_RW              376554 non-null  float64\n",
      " 29  LW_RW_DIFF                 376554 non-null  float64\n",
      " 30  FUEL_IN_TANKS              376554 non-null  float64\n",
      " 31  CALC_VALUE_FOB_DIFF        376554 non-null  float64\n",
      " 32  START_FOB_VS_FOB_FUELUSED  376554 non-null  float64\n",
      " 33  ALTITUDE_DIFF              376554 non-null  float64\n",
      " 34  LEAK_FLOW_FLAG             376554 non-null  int64  \n",
      "dtypes: bool(1), float64(29), int64(2), object(3)\n",
      "memory usage: 98.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Leak Count: 33475\n",
      "Train set leak rate: 8.2841% == Training Leak Count: 26515 (Expected: 28453) \n",
      "Test set leak rate: 12.3221% == Testing Leak Count: 6960 (Expected: 5022)\n",
      "Training Data Shape: (320070, 35), Testing Data Shape: (56484, 35)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test(df, leak_column=\"LEAK_FLOW_FLAG\", test_size=0.15):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train and test sets while maintaining the chronological order.\n",
    "    Ensures the same proportion of leaks in train and test sets.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by=\"UTC_TIME\")  # Sort by time to prevent data leakage\n",
    "\n",
    "    split_idx = int(len(df) * (1 - test_size))  # Compute split index\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    test_df = df.iloc[split_idx:]\n",
    "\n",
    "    # Count the number of leaks in each dataset\n",
    "    train_leak_count = train_df['LEAK_FLOW_FLAG'].sum()\n",
    "    test_leak_count = test_df['LEAK_FLOW_FLAG'].sum()\n",
    "    total_leak_count = df['LEAK_FLOW_FLAG'].sum()\n",
    "    # Compute expected leak split (should be 75%-25%)\n",
    "    expected_train_leaks = int(total_leak_count * 0.85)\n",
    "    expected_test_leaks = total_leak_count - expected_train_leaks\n",
    "\n",
    "    # Display class distributions\n",
    "    print(f\"Total Leak Count: {total_leak_count}\")\n",
    "    print(f\"Train set leak rate: {100 * train_df[leak_column].mean():.4f}% == Training Leak Count: {train_leak_count} (Expected: {expected_train_leaks}) \")\n",
    "    print(f\"Test set leak rate: {100 * test_df[leak_column].mean():.4f}% == Testing Leak Count: {test_leak_count} (Expected: {expected_test_leaks})\")\n",
    "    print(f\"Training Data Shape: {train_df.shape}, Testing Data Shape: {test_df.shape}\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Apply split\n",
    "train_df, test_df = split_train_test(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1ms/step - loss: 0.0629 - val_loss: 0.0329\n",
      "Epoch 2/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0233 - val_loss: 0.0065\n",
      "Epoch 3/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 5/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 6/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 7/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 8/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 9/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 10/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 11/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 12/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 13/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 14/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 15/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0055\n",
      "Epoch 16/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0066\n",
      "Epoch 17/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 18/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 19/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0053\n",
      "Epoch 20/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 21/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0052\n",
      "Epoch 22/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0037 - val_loss: 0.0060\n",
      "Epoch 23/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 24/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0044\n",
      "Epoch 25/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0034 - val_loss: 0.0046\n",
      "Epoch 26/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 27/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 28/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 29/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0053\n",
      "Epoch 30/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 31/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 32/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0053\n",
      "Epoch 33/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 34/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0072\n",
      "Epoch 35/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0051\n",
      "Epoch 36/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 37/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 38/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 39/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 40/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 41/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 42/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 43/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 44/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 45/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 46/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 47/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 48/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0046\n",
      "Epoch 49/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 50/50\n",
      "\u001b[1m7339/7339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "\u001b[1m1766/1766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 931us/step\n",
      "Confusion Matrix:\n",
      "[[46954  2570]\n",
      " [ 6705   255]]\n",
      "Accuracy: 0.8357942072091211\n",
      "Precision: 0.09026548672566372\n",
      "Recall: 0.036637931034482756\n",
      "F1 Score: 0.052120592743995914\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "# Filter out the entries with leaks from training data\n",
    "train_data = train_df[train_df['LEAK_FLOW_FLAG'] == 0].drop(columns=['LEAK_FLOW_FLAG'])\n",
    "\n",
    "# Identify numerical columns and scale only those\n",
    "numerical_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data[numerical_cols])\n",
    "\n",
    "# Define the autoencoder model architecture\n",
    "input_dim = train_data_scaled.shape[1]  # Number of features\n",
    "encoding_dim = 14  # Dimension of the encoded representation\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "# Encoder layers\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "# Decoder layers\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder on the scaled training data\n",
    "autoencoder.fit(train_data_scaled, train_data_scaled,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=False,\n",
    "                validation_split=0.2,\n",
    "                verbose=1)\n",
    "\n",
    "# Prepare the test data by selecting only numerical columns and scaling\n",
    "test_data = test_df[numerical_cols]\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Get the reconstruction loss (MSE) on the test data\n",
    "reconstructions = autoencoder.predict(test_data_scaled)\n",
    "mse = np.mean(np.power(test_data_scaled - reconstructions, 2), axis=1)\n",
    "\n",
    "# Determine the threshold for anomaly detection based on 95th percentile of MSE\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "# Detect anomalies based on the threshold\n",
    "test_df = test_df.copy()  # Avoid SettingWithCopyWarning\n",
    "test_df['reconstruction_error'] = mse\n",
    "test_df['anomaly'] = test_df['reconstruction_error'] > threshold\n",
    "\n",
    "# Calculate evaluation metrics: Confusion Matrix, Accuracy, Precision, Recall, F1 Score\n",
    "y_true = test_df['LEAK_FLOW_FLAG']  # True labels\n",
    "y_pred = test_df['anomaly'].astype(int)  # Predicted labels (0 or 1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)  # Confusion Matrix\n",
    "accuracy = accuracy_score(y_true, y_pred)  # Accuracy\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)  # Precision\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)  # Recall\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)  # F1 Score\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix:\n",
      " [[286800   6755]\n",
      " [ 25268   1247]]\n",
      "Testing Confusion Matrix:\n",
      " [[48027  1497]\n",
      " [ 6861    99]]\n",
      "Accuracy: 0.8520288931378798\n",
      "Recall: 0.014224137931034483\n",
      "Precision: 0.06203007518796992\n",
      "F1 Score: 0.02314165497896213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Remove unwanted columns from training and test data\n",
    "test_df = test_df.drop(columns=['reconstruction_error'], errors='ignore')\n",
    "train_df = train_df.drop(columns=['reconstruction_error'], errors='ignore')\n",
    "\n",
    "# Extract numerical features for training and testing\n",
    "train_x = train_df.select_dtypes(include=[np.number]).drop(columns=['LEAK_FLOW_FLAG'])\n",
    "test_x = test_df.select_dtypes(include=[np.number]).drop(columns=['LEAK_FLOW_FLAG'])\n",
    "\n",
    "# Create an Isolation Forest model\n",
    "isolation_forest = IsolationForest(contamination=0.025, random_state=42)\n",
    "\n",
    "# Fit the model on training data\n",
    "isolation_forest.fit(train_x)\n",
    "\n",
    "# Predict anomalies\n",
    "train_x_predictions = isolation_forest.predict(train_x)\n",
    "test_x_predictions = isolation_forest.predict(test_x)\n",
    "\n",
    "# Convert predictions to binary labels (-1 indicates an anomaly)\n",
    "train_x_predictions = (train_x_predictions == -1).astype(int)\n",
    "test_x_predictions = (test_x_predictions == -1).astype(int)\n",
    "\n",
    "# Print the confusion matrix for training data\n",
    "cm_train = confusion_matrix(train_df['LEAK_FLOW_FLAG'], train_x_predictions)\n",
    "print(\"Training Confusion Matrix:\\n\", cm_train)\n",
    "\n",
    "# Print the confusion matrix for test data\n",
    "cm_test = confusion_matrix(test_df['LEAK_FLOW_FLAG'], test_x_predictions)\n",
    "print(\"Testing Confusion Matrix:\\n\", cm_test)\n",
    "\n",
    "# Print precision, recall, accuracy, and F1 score\n",
    "accuracy = accuracy_score(test_df['LEAK_FLOW_FLAG'], test_x_predictions)\n",
    "recall = recall_score(test_df['LEAK_FLOW_FLAG'], test_x_predictions, zero_division=0)\n",
    "precision = precision_score(test_df['LEAK_FLOW_FLAG'], test_x_predictions, zero_division=0)\n",
    "f1 = f1_score(test_df['LEAK_FLOW_FLAG'], test_x_predictions, zero_division=0)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training Confusion Matrix:\n",
      " [[290314   3241]\n",
      " [ 20008   6507]]\n",
      "Testing Confusion Matrix:\n",
      " [[49312   212]\n",
      " [ 6958     2]]\n",
      "Accuracy: 0.8730613979179945\n",
      "Recall: 0.00028735632183908046\n",
      "Precision: 0.009345794392523364\n",
      "F1 Score: 0.0005575689991636465\n",
      "                     Feature  Importance\n",
      "4  START_FOB_VS_FOB_FUELUSED    0.160377\n",
      "5            TOTAL_FUEL_USED    0.157967\n",
      "8                     lagged    0.145000\n",
      "2             FOB_DIFFERENCE    0.141719\n",
      "1               EXPECTED_FOB    0.140515\n",
      "0                  VALUE_FOB    0.132797\n",
      "7             VALUE_FOB_mean    0.101483\n",
      "3                 FOB_CHANGE    0.020142\n",
      "6         FLIGHT_PHASE_COUNT    0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Feature selection for XGBoost\n",
    "drop_columns = ['UTC_TIME', 'FLIGHT_ID', 'MSN']\n",
    "selected_features = [\n",
    "    'VALUE_FOB', 'EXPECTED_FOB', 'FOB_DIFFERENCE', 'FOB_CHANGE',\n",
    "    'START_FOB_VS_FOB_FUELUSED', 'TOTAL_FUEL_USED', 'FLIGHT_PHASE_COUNT', 'LEAK_FLOW_FLAG'\n",
    "]\n",
    "\n",
    "# Ensure rolling features exist before selecting features\n",
    "train_df['VALUE_FOB_mean'] = train_df['VALUE_FOB'].rolling(window=10).mean()\n",
    "train_df['VALUE_FOB_mean'] = train_df['VALUE_FOB_mean'].fillna(train_df['VALUE_FOB_mean'].mean())\n",
    "train_df['lagged'] = train_df['VALUE_FOB'].shift(1).fillna(train_df['VALUE_FOB'].mean())\n",
    "test_df['VALUE_FOB_mean'] = test_df['VALUE_FOB'].rolling(window=10).mean()\n",
    "test_df['VALUE_FOB_mean'] = test_df['VALUE_FOB_mean'].fillna(test_df['VALUE_FOB_mean'].mean())\n",
    "test_df['lagged'] = test_df['VALUE_FOB'].shift(1).fillna(test_df['VALUE_FOB'].mean())\n",
    "\n",
    "# Add rolling features to selected features\n",
    "selected_features.extend(['VALUE_FOB_mean', 'lagged'])\n",
    "\n",
    "xgboostdata = train_df[selected_features]\n",
    "\n",
    "# Ensure test set contains all necessary features\n",
    "X_train = xgboostdata.drop(columns=['LEAK_FLOW_FLAG'])\n",
    "y_train = xgboostdata['LEAK_FLOW_FLAG']\n",
    "X_test = test_df[selected_features].drop(columns=['LEAK_FLOW_FLAG'], errors='ignore')\n",
    "y_test = test_df['LEAK_FLOW_FLAG']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "train_x_predictions = xgb.predict(X_train_scaled)\n",
    "test_x_predictions = xgb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "cm_train = confusion_matrix(y_train, train_x_predictions)\n",
    "cm_test = confusion_matrix(y_test, test_x_predictions)\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_x_predictions)\n",
    "recall = recall_score(y_test, test_x_predictions, zero_division=0)\n",
    "precision = precision_score(y_test, test_x_predictions, zero_division=0)\n",
    "f1 = f1_score(y_test, test_x_predictions, zero_division=0)\n",
    "\n",
    "print(\"Training Confusion Matrix:\\n\", cm_train)\n",
    "print(\"Testing Confusion Matrix:\\n\", cm_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = xgb.feature_importances_\n",
    "features = X_train.columns\n",
    "feature_importances_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importances_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested HPT in another instance and it only upped the F1 up to 0.0134453"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Hyperparameter tuning\n",
    "tuning_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [2, 4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=XGBClassifier(), param_grid=tuning_params, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Predict with the best model\n",
    "train_x_predictions = grid_search.best_estimator_.predict(X_train_scaled)\n",
    "test_x_predictions = grid_search.best_estimator_.predict(X_test_scaled)\n",
    "\n",
    "# Final evaluation\n",
    "cm_train = confusion_matrix(y_train, train_x_predictions)\n",
    "cm_test = confusion_matrix(y_test, test_x_predictions)\n",
    "accuracy = accuracy_score(y_test, test_x_predictions)\n",
    "recall = recall_score(y_test, test_x_predictions, zero_division=0)\n",
    "precision = precision_score(y_test, test_x_predictions, zero_division=0)\n",
    "f1 = f1_score(y_test, test_x_predictions, zero_division=0)\n",
    "\n",
    "print(\"Final Training Confusion Matrix:\\n\", cm_train)\n",
    "print(\"Final Testing Confusion Matrix:\\n\", cm_test)\n",
    "print(f'Final Accuracy: {accuracy}')\n",
    "print(f'Final Recall: {recall}')\n",
    "print(f'Final Precision: {precision}')\n",
    "print(f'Final F1 Score: {f1}')\n",
    "\n",
    "# Print final feature importance\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importances_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UTC_TIME', 'FUEL_USED_2', 'FUEL_USED_3', 'FUEL_USED_4',\n",
       "       'FW_GEO_ALTITUDE', 'VALUE_FOB', 'VALUE_FUEL_QTY_CT',\n",
       "       'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3',\n",
       "       'VALUE_FUEL_QTY_FT4', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT',\n",
       "       'FLIGHT_PHASE_COUNT', 'FUEL_USED_1', 'Flight', 'MSN', 'NEW_FLIGHT',\n",
       "       'FLIGHT_INSTANCE', 'FLIGHT_ID', 'START_FOB', 'TOTAL_FUEL_USED',\n",
       "       'EXPECTED_FOB', 'FOB_DIFFERENCE', 'FOB_CHANGE', 'EXPECTED_FOB_CHANGE',\n",
       "       'FUEL_LEAK_RATE', 'TOTAL_FUEL_LW', 'TOTAL_FUEL_RW', 'LW_RW_DIFF',\n",
       "       'FUEL_IN_TANKS', 'CALC_VALUE_FOB_DIFF', 'START_FOB_VS_FOB_FUELUSED',\n",
       "       'ALTITUDE_DIFF', 'LEAK_FLOW_FLAG', 'VALUE_FOB_mean', 'lagged'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 6ms/step - loss: 0.0226 - val_loss: 0.0014\n",
      "Epoch 2/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 3/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 8.4629e-04 - val_loss: 0.0017\n",
      "Epoch 4/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 6.9454e-04 - val_loss: 0.0017\n",
      "Epoch 5/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 5.9202e-04 - val_loss: 0.0021\n",
      "Epoch 6/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 5.3071e-04 - val_loss: 0.0023\n",
      "Epoch 7/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 4.9703e-04 - val_loss: 0.0022\n",
      "Epoch 8/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 4.6409e-04 - val_loss: 0.0023\n",
      "Epoch 9/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 9ms/step - loss: 4.3456e-04 - val_loss: 0.0026\n",
      "Epoch 10/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - loss: 4.1288e-04 - val_loss: 0.0026\n",
      "Epoch 11/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 8ms/step - loss: 3.9547e-04 - val_loss: 0.0027\n",
      "Epoch 12/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 3.8397e-04 - val_loss: 0.0026\n",
      "Epoch 13/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 3.6933e-04 - val_loss: 0.0029\n",
      "Epoch 14/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 10ms/step - loss: 3.6231e-04 - val_loss: 0.0028\n",
      "Epoch 15/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 3.4621e-04 - val_loss: 0.0030\n",
      "Epoch 16/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 3.3871e-04 - val_loss: 0.0028\n",
      "Epoch 17/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 3.1997e-04 - val_loss: 0.0030\n",
      "Epoch 18/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 3.1164e-04 - val_loss: 0.0028\n",
      "Epoch 19/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 3.0108e-04 - val_loss: 0.0032\n",
      "Epoch 20/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 2.8998e-04 - val_loss: 0.0032\n",
      "Epoch 21/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 2.7931e-04 - val_loss: 0.0033\n",
      "Epoch 22/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 2.7230e-04 - val_loss: 0.0034\n",
      "Epoch 23/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 2.6712e-04 - val_loss: 0.0035\n",
      "Epoch 24/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 2.6642e-04 - val_loss: 0.0032\n",
      "Epoch 25/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 2.5495e-04 - val_loss: 0.0034\n",
      "Epoch 26/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - loss: 2.4689e-04 - val_loss: 0.0033\n",
      "Epoch 27/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 11ms/step - loss: 2.3870e-04 - val_loss: 0.0035\n",
      "Epoch 28/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 11ms/step - loss: 2.3127e-04 - val_loss: 0.0037\n",
      "Epoch 29/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 13ms/step - loss: 2.3516e-04 - val_loss: 0.0037\n",
      "Epoch 30/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 10ms/step - loss: 2.2448e-04 - val_loss: 0.0037\n",
      "Epoch 31/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 2.1814e-04 - val_loss: 0.0037\n",
      "Epoch 32/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 2.1544e-04 - val_loss: 0.0038\n",
      "Epoch 33/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 2.1517e-04 - val_loss: 0.0036\n",
      "Epoch 34/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - loss: 2.1043e-04 - val_loss: 0.0036\n",
      "Epoch 35/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 2.0487e-04 - val_loss: 0.0034\n",
      "Epoch 36/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 2.0727e-04 - val_loss: 0.0037\n",
      "Epoch 37/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - loss: 1.9943e-04 - val_loss: 0.0036\n",
      "Epoch 38/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 11ms/step - loss: 1.9544e-04 - val_loss: 0.0039\n",
      "Epoch 39/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 1.9165e-04 - val_loss: 0.0036\n",
      "Epoch 40/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 1.8845e-04 - val_loss: 0.0035\n",
      "Epoch 41/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 8ms/step - loss: 1.9034e-04 - val_loss: 0.0036\n",
      "Epoch 42/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - loss: 1.8183e-04 - val_loss: 0.0038\n",
      "Epoch 43/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 1.8536e-04 - val_loss: 0.0038\n",
      "Epoch 44/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 9ms/step - loss: 1.8108e-04 - val_loss: 0.0037\n",
      "Epoch 45/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.7715e-04 - val_loss: 0.0036\n",
      "Epoch 46/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 8ms/step - loss: 1.8172e-04 - val_loss: 0.0038\n",
      "Epoch 47/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 1.8130e-04 - val_loss: 0.0041\n",
      "Epoch 48/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - loss: 1.7103e-04 - val_loss: 0.0037\n",
      "Epoch 49/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.7034e-04 - val_loss: 0.0038\n",
      "Epoch 50/50\n",
      "\u001b[1m4001/4001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.6749e-04 - val_loss: 0.0038\n",
      "\u001b[1m1766/1766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "Confusion Matrix:\n",
      "[[48653   871]\n",
      " [ 5006  1954]]\n",
      "Accuracy: 0.8959528362014022\n",
      "Precision: 0.6916814159292035\n",
      "Recall: 0.2807471264367816\n",
      "F1 Score: 0.399386816555953\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "# Identify numerical columns and scale only those\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_df[numerical_cols])\n",
    "test_data_scaled = scaler.transform(test_df[numerical_cols])\n",
    "\n",
    "# Reshape data for LSTM (samples, timesteps, features)\n",
    "time_steps = 10  # Number of time steps for LSTM input\n",
    "train_data_reshaped = train_data_scaled.reshape((train_data_scaled.shape[0], 1, train_data_scaled.shape[1]))\n",
    "test_data_reshaped = test_data_scaled.reshape((test_data_scaled.shape[0], 1, test_data_scaled.shape[1]))\n",
    "\n",
    "# Define LSTM Autoencoder model\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='relu', input_shape=(1, train_data_scaled.shape[1]), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='relu', return_sequences=False),\n",
    "    RepeatVector(1),\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(train_data_scaled.shape[1]))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the LSTM Autoencoder\n",
    "model.fit(train_data_reshaped, train_data_reshaped,\n",
    "          epochs=50,\n",
    "          batch_size=64,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          verbose=1)\n",
    "\n",
    "# Get reconstruction loss (MSE) on the test data\n",
    "reconstructions = model.predict(test_data_reshaped)\n",
    "mse = np.mean(np.power(test_data_reshaped - reconstructions, 2), axis=(1, 2))\n",
    "\n",
    "# Determine anomaly threshold based on 95th percentile\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "# Detect anomalies based on the threshold\n",
    "test_df = test_df.copy()\n",
    "test_df['reconstruction_error'] = mse\n",
    "test_df['anomaly'] = test_df['reconstruction_error'] > threshold\n",
    "\n",
    "# Evaluation metrics\n",
    "y_true = test_df['LEAK_FLOW_FLAG']\n",
    "y_pred = test_df['anomaly'].astype(int)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
